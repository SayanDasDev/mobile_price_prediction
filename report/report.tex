\documentclass[12pt]{report}
\usepackage[top=1in, bottom=1in, left=0.5in, right=0.5in]{geometry}
\usepackage{amsmath, amsfonts, amssymb, ebgaramond-maths, enumerate, fancyhdr, xcolor, lipsum, titling, minted, algorithm, algpseudocode, algorithmicx, float, hyperref, booktabs, graphicx, array}
\usepackage[utf8]{inputenc}

\usepackage[Glenn]{fncychap}
\usepackage[skip=20pt, indent=30pt]{parskip}
\usepackage{setspace}
\onehalfspacing

\usepackage[some]{background}

\definecolor{titlepagecolor}{cmyk}{.02,.04,0,.90}

\DeclareFixedFont{\bigsf}{T1}{phv}{b}{n}{1.5cm}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
morekeywords={self},              % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false,
tabsize=2,
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


\makeatletter
\renewcommand{\@chapapp}{Section}
\makeatother
\renewcommand{\thechapter}{\Roman{chapter}}

\pagestyle{fancy}
\fancyhead[l]{Final Project Report \\}
\fancyhead[c]{Section \thechapter \\}
\fancyhead[r]{Sayan Das: \texttt{B2430035} \\ Raihan Uddin: \texttt{B2430070}}
\fancyfoot[c]{\thepage}
\renewcommand{\headrulewidth}{0.2pt}
\setlength{\headheight}{28pt}

\title{Final Project Report}
\author{
  Sayan Das
  \and
  Raihan Uddin
}

\begin{document}

\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\center


	\textsc{\LARGE \textbf{Ramakrishna Mission Vivekananda Educational and Research Institute}}\\[1.5cm]

	\textsc{\LARGE Machine Learning}\\[0.5cm]

	\textsc{\large Final Project Report}\\[0.5cm]

	\HRule\\[0.4cm]

	{\huge\bfseries A Classification Based approach for predicting Smartphone Price Categories}\\[0.4cm]

	\HRule\\[1.5cm]



	\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
			\large
			\textit{Submitted By}\\
			\textsc{\textbf{Sayan Das }}\\
			\vspace{-0.5em}
			\textsc{\texttt{B2430035 }}\\
			\textsc{\textbf{Raihan Uddin }}\\
			\vspace{-0.5em}
			\textsc{\texttt{B2430070} }\\
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright}
			\large
			\textit{Submitted To}\\
			\textbf{\textsc{Br. Bhaswarachaitanya (Tamal Maharaj)}}
		\end{flushright}
	\end{minipage}


	\vfill\vfill\vfill

	{\large November 25, 2024}

	\vfill

\end{titlepage}
\restoregeometry

\tableofcontents

\chapter{Introduction}
\section{Background}
Now a days, smartphones are more than just a means of communication. The global smartphone market is characterized by rapid technological innovation, intense competition, and increasingly sophisticated consumer expectations. With the proliferation of smartphones, the market dynamics have become increasingly complex, driven by continuous technological advancements, changing consumer preferences, and competitive pricing strategies.

The smartphone industry represents a highly dynamic technological ecosystem where manufacturers constantly strive to differentiate their products through innovative features, design, and pricing. Each season, hundreds of new smartphones are launched, each targeting different market segments and consumer needs. This rapid evolution creates significant challenges for both manufacturers and consumers in understanding and predicting smartphone pricing.

From a manufacturer's perspective, optimal pricing is crucial for maintaining market competitiveness and profitability. Pricing strategies must balance multiple factors including technological features, production costs, market positioning, and consumer purchasing power. Inaccurate pricing can lead to significant market share losses or reduced profit margins.

For consumers, purchasing a smartphone requires understanding of the complex landscape of technical specifications, brand reputation, and market trends. The ability to predict or understand the factors influencing smartphone prices can help consumers make more informed purchasing decisions and assess the value proposition of different devices.

The emergence of machine learning techniques offers promising approaches to address these pricing challenges. By leveraging historical data and advanced predictive modeling, it becomes possible to develop more sophisticated and accurate methods of smartphone price categorization and prediction.
\section{Motivation}
The motivation for this research stems from the increasingly complex and dynamic nature of the smartphone market. Several critical challenges drive the need for an advanced smartphone price categorization approach:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{Economic Significance for Manufacturers :}} Inaccurate pricing can result in significant financial losses or missed market opportunities.
	\item{\textbf{Tranceparent Pricing for Consumers :}} Many consumers face challenges in understanding the intrinsic value of smartphones. A data-driven approach to price categorization can provide transparent insights into the factors that genuinely influence smartphone pricing.
\end{enumerate}
\section{Objectives}
The primary objectives of this project are:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{Develop a Robust Classification Model :}}
		\vspace{-1.65em}
		\begin{itemize}
			\setlength\itemsep{-1.5em}
			\item Create a machine learning model capable of accurately categorizing smartphones into distinct price segments
			\item Achieve high predictive accuracy using multiple classification algorithms
			\item Identify and leverage the most significant features influencing smartphone pricing
		\end{itemize}
	\item{\textbf{Feature Analysis and Selection :}}
		\vspace{-1.65em}
		\begin{itemize}
			\setlength\itemsep{-1.5em}
			\item Conduct comprehensive analysis of smartphone features
			\item Determine the most influential factors in price categorization
			\item Develop a systematic approach to feature selection and importance ranking
		\end{itemize}
	\item{\textbf{Comparative Algorithm Performance :}}
		\vspace{-1.65em}
		\begin{itemize}
			\setlength\itemsep{-1.5em}
			\item Implement and evaluate multiple machine learning algorithms
			\item Compare the performance of different classification techniques
			\item Identify the most effective algorithm for smartphone price category prediction
		\end{itemize}
	\item{\textbf{Practical Applicability :}}
		\vspace{-1.65em}
		\begin{itemize}
			\setlength\itemsep{-1.5em}
			\item Demonstrate the practical utility of the developed model for both manufacturers and consumers
		\end{itemize}
	\item{\textbf{Methodological Contribution :}}
		\vspace{-1.65em}
		\begin{itemize}
			\setlength\itemsep{-1.5em}
			\item Develop a systematic approach to smartphone price categorization
			\item Contribute to the existing body of knowledge in machine learning applications in market analysis
			\item Establish a replicable methodology for similar predictive modeling challenges
		\end{itemize}
\end{enumerate}

\chapter{Literature Review}
\begin{quotation}
  In this paper Asim et al. \cite{asim2018} (2018) investigates price class prediction for smartphones using machine learning, focusing on determining whether a smartphone is economical or expensive. Data was collected from GSMArena, including features like display size, weight, RAM, and battery capacity. The dataset was preprocessed and categorized into four price classes: very economical, economical, expensive, and very expensive. Feature selection methods like InfoGain and WrapperAttributeEval were employed to reduce dataset dimensionality, optimizing computational efficiency. The study tested classifiers such as Decision Tree (J48) and Naïve Bayes, achieving a maximum accuracy of 78\% with Decision Tree when combined with WrapperAttributeEval. Challenges included converting a regression problem into classification, which introduced errors, and the limited dataset size impacting accuracy. The paper concludes with recommendations for improved feature selection techniques, larger datasets, and extending the model for other product categories.
\end{quotation}

\begin{quotation}
  Chandrashekhara et al. \cite{chandrashekhara2019} (2019), focuses on predicting smartphone prices using machine learning techniques like Support Vector Regression (SVR), Backpropagation Neural Network (BNN), and Multiple Linear Regression (MLR). The dataset included features such as brand, RAM, memory, battery power, and display size, with 262 records spanning 2010–2018, sourced from e-commerce platforms. Data preprocessing steps included handling missing values, standardizing formats, and splitting the data into training (80\%) and testing (20\%) sets. Performance metrics such as R-squared and correlation values were used to evaluate models. SVR performed the best with an R-squared of 0.86 and correlation of 0.93. Graphical analyses demonstrated that SVR had the most accurate predictions, followed by BNN and MLR. The authors highlight the potential of SVR for broader applications in price prediction across retail industries, suggesting scalability through distributed systems like Hadoop.
\end{quotation}

\begin{quotation}
  Ercan \& Şimşek et al. \cite{ercan2023} (2023) examines the classification of smartphone prices into low, medium, high, and very high categories using a Kaggle dataset with 2000 entries and 20 features. Features included physical attributes (weight, dimensions), performance metrics (RAM, processor cores), and functionalities (3G, Wi-Fi). Four machine learning algorithms—Logistic Regression, Support Vector Machine (SVM), Decision Tree, and K-Nearest Neighbors (KNN)—were applied and evaluated. SVM achieved the highest accuracy (96.16\%), outperforming Logistic Regression (91\%), Decision Tree (82\%), and KNN (41\%). Confusion matrices and performance metrics (accuracy, precision, recall, and F1-score) were used to validate results. The study emphasizes the superiority of SVM for this classification problem and suggests exploring additional algorithms and datasets for improved accuracy in future work.
\end{quotation}

\begin{quotation}
  In another study, Abbasi et al. \cite{abbasi2024} (2024) focuses on predicting the price of second-hand electronic devices, particularly smartphones, using machine learning (ML) techniques. It addresses the growing trend of buying used electronics, driven by economic factors and technological advancements. The authors utilized web scraping to gather a dataset covering the last five years, containing various features influencing smartphone prices. They experimented with three ML algorithms—Random Forest, Linear Regression, and Multi-Layer Perceptron—to develop a predictive model. Performance evaluation was based on metrics like Absolute Percentage Difference (APD) and Root Mean Square Error (RMSE). Random Forest emerged as the best-performing model, demonstrating the lowest prediction error and superior generalization. The study highlights how accurate price prediction can aid both buyers and sellers, making the market more efficient. Data preprocessing and feature selection were critical steps, leading to a dataset with 24 variables. Techniques like 10-fold cross-validation were used to ensure robustness. The findings suggest that incorporating modern ML methods can significantly enhance price prediction accuracy compared to traditional approaches.
\end{quotation}

\begin{quotation}
  In this paper, Akash et al. \cite{gupta2020} (2020) explores smartphone price prediction using machine learning, aiming to categorize smartphones as economical or expensive based on their features. The study emphasizes the relevance of predictive modeling in aiding marketing and consumer decision-making. Key features considered include processor type, battery capacity, memory, and screen size. The authors employed algorithms like Linear Regression and K-Nearest Neighbors (KNN) to build predictive models. The seven-step machine learning process involved data gathering, preparation, model selection, training, evaluation, hyperparameter tuning, and prediction. The KNN model showed higher accuracy compared to Linear Regression, particularly in scenarios involving complex, non-linear relationships. The research demonstrated the importance of feature selection to reduce computational complexity and enhance prediction precision. The conclusion underscores the impact of ML-driven price prediction in optimizing marketing strategies and enhancing business decisions.
\end{quotation}

\begin{quotation}
  Finally, this paper Kumuda S et al. \cite{kumuda2021} (2021) investigates the prediction of smartphone prices using machine learning, with a focus on market analysis and consumer behavior. The study outlines the challenges of determining accurate smartphone prices due to diverse features and competitive market dynamics. A dataset containing information on screen size, memory, camera quality, and battery was used to train predictive models. Key algorithms employed included KNN, Forward Selection, and Backward Selection to manage data complexity and optimize feature selection. The data collection phase involved gathering specifications from various smartphones to construct a comprehensive dataset. The authors emphasized the role of data visualization techniques, like the Elbow Method, to identify the optimal number of features. The study presented a detailed analysis of feature importance, highlighting key parameters influencing price prediction. Classification and testing phases assessed the model's accuracy, using preprocessed datasets for training and evaluation. Results were visualized using graphs that compared predicted prices against specifications like RAM and memory. The conclusion suggests that precise feature selection can improve predictive accuracy and aid in product launch decisions.
\end{quotation}


\chapter{Dataset Description}
\section{Source}
The dataset for this smartphone price classification project was obtained from Kaggle, a prominent platform for data science and machine learning datasets.\\ \\
Link - \href{https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification?select=train.csv}{https://www.kaggle.com/datasets/iabhishekofficial/smartphone-price-classification}\\ \\
The dataset is publicly available and contains 2000 smartphone entries with 20 feature variables and a target variable representing price range.
\section{Features}
The dataset comprises 20 features that describe various characteristics of smartphones. Following table provides a detailed breakdown of these features:
\begin{table}[H]
	\begin{tabular}{lll}
		\toprule
		\textbf{Feature Name}   & \textbf{Description}  & \textbf{Type}  \\
		\toprule
		\textbf{battery\_power} & battery capacity in mAh   & Numerical  \\
		\midrule
		\textbf{blue}           & has bluetooth or not  & Binary   \\
		\midrule
		\textbf{clock\_speed}   & speed at which processor executes instructions & Numerical \\
		\midrule
		\textbf{dual\_sim}      & has dual sim support or not & Binary \\
		\midrule
		\textbf{fc}             & front Camera Megapixels & Numerical \\
		\midrule
		\textbf{pc}             & primary Camera Megapixels & Numerical \\
		\midrule
		\textbf{four\_g}        & has 4G or not & Binary \\
		\midrule
		\textbf{three\_g}       & has 3G or not & Binary \\
		\midrule
		\textbf{wifi}           & has wifi or not & Binary \\
		\midrule
		\textbf{int\_memory}    & internal Memory capacity & Numerical \\
		\midrule
		\textbf{m\_dep}         & smartphone Depth in cm & Numerical \\
		\midrule
		\textbf{mobile\_wt}     & weight of the smartphone & Numerical \\
		\midrule
		\textbf{n\_cores}       & number of cores in processor & Numerical \\
		\midrule
		\textbf{px\_height}     & pixel Resolution Height & Numerical \\
		\midrule
		\textbf{px\_width}      & pixel Resolution Width & Numerical \\
		\midrule
		\textbf{ram}            & RAM in MB & Numerical \\
		\midrule
		\textbf{touch\_screen}  & has touch screen or not & Binary \\
		\midrule
		\textbf{sc\_h}          & screen Height in cm & Numerical \\
		\midrule
		\textbf{sc\_w}          & screen Width in cm & Numerical \\
		\midrule
		\textbf{talk\_time}     & longest time that a single battery charge will last over a call & Numerical \\
		\bottomrule
	\end{tabular}
\end{table}
Among these features, some are binary variables (e.g., blue, dual\_sim, four\_g, three\_g touch\_screen, wifi), while others are continuous variables (e.g., battery\_power, clock\_speed, int\_memory, ram, talk\_time). These features collectively provide a comprehensive overview of the smartphone's technical specifications and capabilities.

A histogram visualization of all individual features is given below:
\begin{figure}[H]
	\centering
	\includegraphics[height=25em]{all_feature_viz.png}
	\caption{Histogram visualization of all features.}
	\label{all_feature_viz}
\end{figure}
This visualization give us insight into the distributions of the features. An important observation here is that px\_height can be normalized. Most of the categorical features are balanced except three\_g.

\section{Target Variable}
The target variable, price\_range, represents the categorization of smartphones into different price segments. It is a categorical variable with four distinct classes:
\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.05em}
	\item{\texttt{0 :}} Low-cost smartphones
	\item{\texttt{1 :}} Medium-low cost smartphones
	\item{\texttt{2 :}} Medium-high cost smartphones
	\item{\texttt{3 :}} High-end, premium smartphones
\end{itemize}

\chapter{Data Preprocessing}
\section{Data Cleaning}
\subsection{Handling Missing Values}
A check for missing values - \pythoninline{df.isnull().sum()} revealed no missing values in the dataset.
\subsection{Handling Duplicate Values}
Also, the dataset was checked for duplicate entries - \pythoninline{df.duplicated().sum()}. No duplicate entries were found.
\subsection{Handling Invalid Values}
The dataset was checked for negative entries and none were found in the dataset.
There are some features which can not be zero, like battery\_power, ram, etc. So, we checked for zero values in these features.
We saw px\_height and sc\_w are have 2 and 180 zero values respectively. We will replace these zero values with the mean of the respective features.

Note that fc and pc are numerical yet zero values are not invalid. They can be zero if the phone does not have a front or primary camera.

\section{Outlier Handling}
Outliers can significantly impact the performance of machine learning models. To identify and handle outliers, box plots were generated for each feature to visualize the distribution of data points. \\
\begin{figure}[H]
	\centering
	\includegraphics[height=25em]{box_plots_before.png}
	\caption{Box plots of features before outlier handling.}
	\label{box_plots_before}
\end{figure}
The box plots revealed outliers in two features, fc and px\_height. To address these outliers, the IQR (Interquartile Range) method was used to detect and remove extreme values.
\begin{figure}[H]
	\centering
	\includegraphics[height=25em]{box_plots_after.png}
	\caption{Box plots of features after outlier handling.}
	\label{box_plots_after}
\end{figure}
\section{Class Imbalance Check}
Class imbalance can affect the performance of classification models. To check for class imbalance, the distribution of the target variable was visualized using a count plot. 

We saw that the dataset was balanced across all four price range categories.
\begin{figure}[H]
	\centering
	\includegraphics[height=10em]{imbalance_check.png}
	\caption{Class imbalance check for target variable.}
	\label{imbalance_check}
\end{figure}
The count plot revealed that the dataset was balanced across all four price range categories, with an equal distribution of samples in each class.
\section{Correlation Analysis}
A correlation matrix was generated to identify the relationships between different features and the target variable. This analysis helped identify the most influential features in predicting smartphone prices. The correlation matrix was visualized using a heatmap to provide a clear overview of feature relationships.
\begin{figure}[H]
	\centering
	\includegraphics[height=35em]{corr_viz.png}
	\caption{Correlation heatmap of features.}
	\label{corr_viz}
\end{figure}
Here we saw that ram is highly correlated with price range. There are some features which are correlated with each other like:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{3G and 4G :}} A high correlation here suggests that devices with 4G almost always support 3G, making one of these features redundant.
	\item{\textbf{fc and pc :}} These features are correlated, as better primary cameras often accompany better front cameras.
	\item{\textbf{px\_height and px\_width :}} These are components of screen resolution and are naturally correlated.
	\item{\textbf{sc\_h and sc\_w :}} These are also naturally correlated.
\end{enumerate}
These correlations will be taken into note during next steps to reduce feature redundancy and improve model performance.
\section{Feature Engineering}
Feature engineering involved creating new features from existing ones to enhance the dataset's predictive power. The goal was to derive meaningful attributes that capture relationships between variables and optimize the dataset for machine learning model training. Two new features were introduced based on the existing data:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{px\_area :}} The total pixel area of the smartphone screen, calculated as the product of pixel resolution height (px\_height) and pixel resolution width (px\_width).
	\item{\textbf{screen\_area :}} The physical screen area of the smartphone, calculated as the product of screen height (sc\_h) and screen width (sc\_w).
\end{enumerate}
After deriving the new features, the original components (px\_height, px\_width, sc\_h, sc\_w) were removed from the dataset:
Next, based on correlation analysis, the following redundant features were removed:
\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.05em}
	\item A high correlation exists between 3G and 4G, as devices with 4G generally support 3G. The 4G feature was retained, and 3G was removed.
	\item Front and primary cameras are correlated, as primary cameras often accompany front cameras. The pc feature was retained, and fc was removed.
\end{itemize}

The correlation matrix after feature engineering is as follows:
\begin{figure}[H]
	\centering
	\includegraphics[height=35em]{corr_viz_after_fe.png}
	\caption{Correlation heatmap of features.}
	\label{corr_viz_after_feature_engineering}
\end{figure}

\section{Feature Selection}
We preformed feature selection to identify the most relevant features for predicting smartphone prices. This step involved analyzing feature importance scores from machine learning models, conducting correlation analysis, and using domain knowledge to select the most influential features. Feature selection is aimed to reduce model complexity and improve prediction accuracy.

\noindent In our project, we used the ANOVA F-test (Analysis of Variance) method to evaluate each feature's relationship with the target variable, price\_range, and select features that are statistically significant. The threshold for selection is a p-value of less than $0.1$, indicating a $90\%$ confidence level. \\
The \pythoninline{SelectKBest} method from \pythoninline{sklearn.feature_selection} is used with the scoring function \pythoninline{f_classif}, which applies the ANOVA F-test. \\
Initially, \pythoninline{k='all'} is set to retain all features for evaluation, allowing for the computation of statistical metrics across all features. \\
After fitting the model, the p-values and F-scores for each feature are extracted. These metrics help assess the significance and contribution of each feature to the target variable. \\
Features with a p-value less than $0.05$ are considered statistically significant and are selected for further analysis. \\
The selected features are: \\
\pythoninline{['battery_power', 'int_memory', 'mobile_wt', 'n_cores', 'ram', 'px_area', 'screen_area']} \\
These are then used for model training and evaluation to determine their impact on prediction accuracy and model performance.


\section{Normalizing features}
A historam plot of the features after feature selection was done as follows:
\begin{figure}[H]
	\centering
	\includegraphics[height=25em]{nrmlz_plots.png}
	\caption{Histogram visualization of all features after feature selection}
	\label{all_feature_viz_after_feature_selection}
\end{figure}
\noindent As we can see the px\_area and screen area can be normalized. \\
We will normalize these features using \pythoninline{PowerTransformer()}.
\begin{figure}[H]
	\centering
	\includegraphics[height=10em]{nrmlz_px_ba.png}
	\caption{Histogram visualization of px\_area before and after normalization}
	\label{hist_viz_px_area_norm}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[height=10em]{nrmlz_screen_ba.png}
	\caption{Histogram visualization of screen\_area before and after normalization}
	\label{hist_viz_screen_area_norm}
\end{figure}

\section{Train-Test Split}
\label{sec:train_test_split}
The dataset was split into training and testing sets to evaluate model performance. The training set was used to train the machine learning models, while the testing set was used to assess model accuracy and generalization. The split ratio was 80\% training and 20\% testing to ensure an adequate balance between model training and evaluation. We used sklearn's \pythoninline{train_test_split} function to perform the split.
\section{Scaling}
The dataset was scaled using the \pythoninline{StandardScaler} from \pythoninline{sklearn.preprocessing} to normalize the feature values. Scaling ensures that all features contribute equally to the model training process and prevents any feature from dominating the others. The scaled dataset was used for model training and evaluation to improve prediction accuracy and model performance.

\chapter{Methodology}
\section{Algorithms Used}
For this project, we experimented with several machine learning algorithms to predict smartphone price categories. The following algorithms were implemented and evaluated:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	
	\item{\textbf{Logistic Regression: }}Logistic Regression allows us to predict the probability of the dependent variable from a given set of independent variables. The probability values are between 0 and 1. \cite{hadsonml}
	\item{\textbf{K-Nearest Neighbors: }}
	K-Nearest Neighbors is a well-known classification technique that bases predictions on finding the closest neighbors in classes that share a lot of characteristics. Since the dataset is scanned one by one to find the nearest neighbors, the performance of the algorithm decreases. It is also known as the lazy learning method. It works slowly in large volumes of data. \cite{hadsonml}
	\item{\textbf{Random Forest: }}Random Forest is a machine learning algorithm that creates a large number of decision trees and combines their predictions. It is a type of ensemble learning method that can be used for classification and regression. Random Forest is a versatile algorithm that can be used for a variety of tasks, including classification, regression, and feature selection. \cite{hadsonml}
	\item{\textbf{Decision Tree: }}There are decision and leaf nodes according to the goal and independent variables in the decision tree algorithm. Because it is a categorization technique that produces a tree-like structure, it is known as a decision tree. The data set entries are processed into a tree to be used for classification, after which the classification procedure is carried out. Algorithms can follow different paths in the selection of root, node and branching criteria. \cite{hadsonml}
	\item{\textbf{Support Vector Machine: }}In Support Vector Machine, the data is separated into two classes and placed on a plane known as the marginal plane. Points from two classes that are adjacent to the line are called support vectors. The goal of Support Vector Machine is to choose the marginal plane with the greatest separation between two data points. This classifier is called a linear support vector machine if the training data can be separated linearly, albeit coarsely. If the data are not linearly separated, it is preferable to use kernel methods and soft range maximization to obtain nonlinear support vector machines. \cite{hadsonml}
	\item{\textbf{XGBoost: }}XGBoost is a scalable and accurate implementation of gradient boosting machines. It is an efficient and effective algorithm that can be used for regression, classification, ranking, and user-defined prediction problems. XGBoost is known for its speed and performance, making it a popular choice for machine learning tasks. \cite{hadsonml}
\end{enumerate}

\section{Justification}
The selection of these algorithms was based on their suitability for classification tasks, performance in handling multi-class classification problems, and ability to capture complex relationships in the data. The justification behind using each of these algorithms are as follows:
\vspace{-1.25em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{Logistic Regression: }}Logistic Regression serves as a great starting point for classification tasks. For predicting price ranges, it can handle categorical output well and provides insights into which features (e.g., RAM or battery power) contribute most to the classification. This helps establish a baseline performance for comparison with other models.
	\item{\textbf{K-Nearest Neighbors: }}KNN is effective in scenarios where decision boundaries are non-linear, as is often the case with diverse smartphone features like camera quality and battery power. For this project, it helps to explore the relationship between clusters of data points and their price ranges.
	\item{\textbf{Random Forest: }}Random Forest excels in handling mixed data types (e.g., numerical and categorical) and mitigating overfitting. In this project, its feature importance analysis can identify critical smartphone features influencing price ranges, such as internal storage or RAM.
	\item{\textbf{Decision Tree: }}Decision Trees provide an interpretable model for understanding hierarchical decision-making based on features. For this project, it illustrates how smartphone features interact to influence price categories, aiding feature selection and refinement.
	\item{\textbf{Support Vector Machine: }}SVM can capture complex relationships between features using kernel tricks, making it suitable for handling overlapping data distributions. Given the challenge of imbalanced class distributions, SVM with proper class weighting can improve prediction accuracy.
	\item{\textbf{XGBoost: }}XGBoost’s advanced boosting mechanism is ideal for capturing intricate patterns in the data. For this project, it addresses imbalanced classes effectively through weight adjustment, delivering state-of-the-art performance on complex datasets.
\end{enumerate}

\chapter{Implementation}
\section{Tools and Libraries}
The tools and libraries used for this project are:
\vspace{-1.25em}
\begin{enumerate}
    \setlength\itemsep{-1.05em}
    \item{\textbf{Python:}} The primary programming language for this project.
    \item{\textbf{Jupyter Notebook:}} For interactive code development.
    \item{\textbf{Data Handling Libraries:}}
        \vspace{-1.5em}
        \begin{enumerate}
            \setlength\itemsep{-1.5em}
            \item{\textbf{Pandas:}} For data manipulation, cleaning, and handling structured datasets.
            \item{\textbf{Numpy:}} For numerical operations, array manipulations, and faster mathematical computations.
        \end{enumerate}
    \item{\textbf{Visualization Libraries: }}
        \vspace{-1.5em}
        \begin{enumerate}
            \setlength\itemsep{-1.5em}
            \item{\textbf{Matplotlib:}} For creating static visualizations like bar charts and line graphs etc.
            \item{\textbf{Seaborn:}} For advanced visualizations, particularly heatmaps and confusion matrices.
        \end{enumerate}
    \item{\textbf{Machine Learning Libraries: }}
        \vspace{-1.5em}
        \begin{enumerate}
            \setlength\itemsep{-1.5em}
            \item{\textbf{Scikit-learn:}} For building, training, and evaluating machine learning models (Logistic Regression, KNN, Random Forest, Decision Tree, Voting Classifier, and SVM). It has also been used for data preprocessing, data splitting, and feature selection.
            \item{\textbf{XGBoost:}} For building and training gradient boosting models.
        \end{enumerate}
    \item{\textbf{Other Libraries: }}
        \vspace{-1.5em}
        \begin{enumerate}
            \setlength\itemsep{-1.5em}
            \item{\textbf{TQDM:}} For showing progress bars during loops.
            \item{\textbf{Time:}} For tracking the time taken by different parts of the code.
        \end{enumerate}
\end{enumerate}

\section{Parameters}
At first, the models were initialized with default parameters from their respective classes in Scikit-learn and XGBoost. Notably:
\vspace{-1.5em}
\begin{enumerate}
	\setlength\itemsep{-1.5em}
	\item{\textbf{Logistic Regression:}} \pythoninline{max_iter=1000} was set to ensure convergence during optimization.
	\item{\textbf{SVM:}} \pythoninline{probability=True} was used to enable probabilistic predictions for compatibility with ensemble models.
	\item{\textbf{XGBoost:}} Used \pythoninline{eval_metric='mlogloss'} as the evaluation metric.
\end{enumerate}
The initial configurations resulted in good training accuracy but some inconsistencies in test performance. Hence hyperparameter tuning was performed to optimize the models for better generalization and prediction accuracy.

\noindent The best configuration we got from hyperparameter tuning is as follows:
\begin{table}[H]
    \centering
    \begin{tabular}{p{3cm}p{8cm}p{1.5cm}}
    \toprule
    \textbf{Model} & \textbf{Best Parameters} & \textbf{Best Score} \\
    \toprule
    Logistic Regression & \{'classifier\_\_C': 10, 'classifier\_\_max\_iter': ...\} & $0.9369$ \\
    \midrule
    KNN & \{'classifier\_\_n\_neighbors': 7, 'classifier\_\_p': ...\} & $0.7740$ \\
    \midrule
    Random Forest & \{'classifier\_\_max\_depth': None, 'classifier\_\_m': ...\} & $0.8946$ \\
    \midrule
    Decision Tree & \{'classifier\_\_criterion': 'gini', 'classifier\_\_': ...\} & $0.8567$ \\
    \midrule
    SVM & \{'classifier\_\_C': 10, 'classifier\_\_gamma': 'sc': ...\} & $0.9350$ \\
    \midrule
    XGBoost & \{'classifier\_\_learning\_rate': 0.1, 'classifier\_\_': ...\} & $0.8977$ \\
    \toprule
    \end{tabular}
    \caption{Best Parameters and Scores for each Model}
		\label{tab:best_params_scores}
\end{table}
The best score represents the highest cross-validation score achieved by each model during hyperparameter tuning. This score is typically the mean accuracy (or another evaluation metric) obtained from cross-validation, indicating how well the model is expected to perform on unseen data with the best-found hyperparameters. \\

\section{Training Process}
As discussed in Section \ref{sec:train_test_split}, the dataset was split into training and testing sets. \\
The training set was used to train various machine learning models, while the testing set was used to evaluate their performance. The models included Logistic Regression, K-Nearest Neighbors (KNN), Random Forest, Decision Tree, Support Vector Machine (SVM), and XGBoost. \\
Each model was trained using the scaled training data and then evaluated on the scaled test data to assess their accuracy, precision, recall, and F1-score. The training process involved fitting the models to the training data and making predictions on both the training and test data. The predictions were compared with the actual labels to calculate the evaluation metrics. This process was repeated for each model to compare their performance and identify the best model for predicting smartphone prices. \\
After this the models were hyperparameter tuned using \pythoninline{GridSearchCV} to get the best parameters for each model. The models were then retrained using the best parameters and evaluated again to get the final performance metrics.

The results were stored in a DataFrame for better visualization and analysis.

\chapter{Results}
\section{Plots}

In this section, we present the confusion matrices, learning curves and ROC-AUC curves for the various machine learning models used in this study.

Learning curves illustrate the model's performance on the training and validation sets as the training size increases. This helps us understand how well the model generalizes to unseen data and whether it is overfitting or underfitting.

Confusion matrices provide a detailed breakdown of the model's performance by showing the number of true positive, true negative, false positive, and false negative predictions. This allows us to gain deeper insights into the strengths and weaknesses of each model, particularly in terms of their ability to correctly classify instances of each class.

ROC-AUC curves provide a visual representation of the model's true positive rate against the false positive rate, helping us evaluate the model's performance across different thresholds.

Precision-Recall curves illustrate the trade-off between precision and recall for different threshold values, providing insights into the model's ability to balance false positives and false negatives.

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_LR.png}
			\caption{Learning Curve for Logistic Regression before Hyperparameter Tuning}
			\label{lc_LR}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_LR.png}
			\caption{Learning Curve for Logistic Regression after Hyperparameter Tuning}
			\label{lc_tuned_LR}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_KNN.png}
			\caption{Learning Curve for K-Nearest Neighbors (KNN) before Hyperparameter Tuning}
			\label{lc_KNN}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_KNN.png}
			\caption{Learning Curve for K-Nearest Neighbors (KNN) after Hyperparameter Tuning}
			\label{lc_tuned_KNN}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_RF.png}
			\caption{Learning Curve for Random Forest before Hyperparameter Tuning}
			\label{lc_RF}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_RF.png}
			\caption{Learning Curve for Random Forest after Hyperparameter Tuning}
			\label{lc_tuned_RF}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_DT.png}
			\caption{Learning Curve for Decision Tree before Hyperparameter Tuning}
			\label{lc_DT}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_DT.png}
			\caption{Learning Curve for Decision Tree after Hyperparameter Tuning}
			\label{lc_tuned_DT}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_SVM.png}
			\caption{Learning Curve for Support Vector Machine (SVM) before Hyperparameter Tuning}
			\label{lc_SVM}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_SVM.png}
			\caption{Learning Curve for Support Vector Machine (SVM) after Hyperparameter Tuning}
			\label{lc_tuned_SVM}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_XG.png}
			\caption{Learning Curve for XGBoost before Hyperparameter Tuning}
			\label{lc_XG}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{lc_tuned_XG.png}
			\caption{Learning Curve for XGBoost after Hyperparameter Tuning}
			\label{lc_tuned_XG}
	\end{minipage}
\end{figure}
$ $ \\
By analsing these learning curves we infer the following:
\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.5em}
		\item \textbf{Logistic Regression}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{Before Tuning}: Moderate gap between training and validation accuracy, with validation scores plateauing slightly lower.
					\item \textbf{After Tuning}: Reduced gap, with validation accuracy closely tracking training accuracy, suggesting a well-generalized model.
			\end{itemize}
		\item \textbf{KNN}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{Before Tuning}: Significant overfitting is evident as training accuracy is very high while validation accuracy lags.
					\item \textbf{After Tuning}: Validation accuracy improves significantly, and the gap narrows, suggesting that the optimal number of neighbors was found.
				\end{itemize}
		\item \textbf{Random Forest}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: Overfitting is apparent; training accuracy is high, but validation accuracy is much lower and plateaus early.
        \item \textbf{After Tuning}: Validation accuracy improves substantially, with a narrower gap between training and validation curves, showing better generalization.
			\end{itemize}
		\item \textbf{Decision Tree}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: Severe overfitting with perfect training accuracy but low validation accuracy.
        \item \textbf{After Tuning}: Validation accuracy improves considerably, with a more realistic and generalizable training score.
			\end{itemize}
		\item \textbf{SVM}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: Moderate gap between training and validation accuracy; validation scores plateau at a suboptimal level.
        \item \textbf{After Tuning}: Narrowed gap and improved validation accuracy indicate better parameter selection.
			\end{itemize}
		\item \textbf{XGBoost}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: Overfitting is evident as training accuracy is high, but validation accuracy is lower.
        \item \textbf{After Tuning}: Dramatic improvement in validation accuracy and narrowing of the gap indicate effective hyperparameter tuning.
    \end{itemize}
\end{itemize}

Next we will look at the confusion matrices.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_LR.png}
        \caption{Confusion Matrix for Logistic Regression before Hyperparameter Tuning}
        \label{cmat_LR}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_LR.png}
        \caption{Confusion Matrix for Logistic Regression after Hyperparameter Tuning}
        \label{cmat_tuned_LR}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{cmat_KNN.png}
			\caption{Confusion Matrix for K-Nearest Neighbors (KNN) before Hyperparameter Tuning}
			\label{cmat_KNN}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_KNN.png}
        \caption{Confusion Matrix for K-Nearest Neighbors (KNN) after Hyperparameter Tuning}
        \label{cmat_tuned_KNN}
    \end{minipage}
    
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{cmat_RF.png}
			\caption{Confusion Matrix for Random Forest before Hyperparameter Tuning}
			\label{cmat_RF}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_RF.png}
        \caption{Confusion Matrix for Random Forest after Hyperparameter Tuning}
        \label{cmat_tuned_RF}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{cmat_DT.png}
			\caption{Confusion Matrix for Decision Tree before Hyperparameter Tuning}
			\label{cmat_DT}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_DT.png}
        \caption{Confusion Matrix for Decision Tree after Hyperparameter Tuning}
        \label{cmat_tuned_DT}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{cmat_SVM.png}
			\caption{Confusion Matrix for Support Vector Machine (SVM) before Hyperparameter Tuning}
			\label{cmat_SVM}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_SVM.png}
        \caption{Confusion Matrix for Support Vector Machine (SVM) after Hyperparameter Tuning}
        \label{cmat_tuned_SVM}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{cmat_XG.png}
			\caption{Confusion Matrix for XGBoost before Hyperparameter Tuning}
			\label{cmat_XG}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{cmat_tuned_XG.png}
        \caption{Confusion Matrix for XGBoost after Hyperparameter Tuning}
        \label{cmat_tuned_XG}
    \end{minipage}
\end{figure}
$ $\\
\noindent By analyzing these confusion matrices, we can infer the following:

\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.5em}
		\item \textbf{Logistic Regression}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{Before Tuning}: Consistent performance with high true positives and low false positives.
					\item \textbf{After Tuning}: Slight improvement in true positives for Class 1 and Class 3, indicating a well-optimized model.
			\end{itemize}
		\item \textbf{KNN}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{Before Tuning}: Moderate performance with higher false positives, especially for Class 1 and Class 2.
					\item \textbf{After Tuning}: Significant improvement in reducing false positives for Class 0 and Class 1, indicating effective hyperparameter tuning.
				\end{itemize}
		\item \textbf{Random Forest}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: High true positives but some false positives, particularly for Class 1 and Class 2.
        \item \textbf{After Tuning}: Slight changes, indicating that the model was already performing well.
			\end{itemize}
		\item \textbf{Decision Tree}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: High true positives but some false positives, particularly for Class 1 and Class 2.
        \item \textbf{After Tuning}: Slight changes, indicating that the model was already performing well.
			\end{itemize}
		\item \textbf{SVM}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: Consistent performance with high true positives and low false positives.
        \item \textbf{After Tuning}: Slight improvement in true positives for Class 1 and Class 3, indicating a well-optimized model.
			\end{itemize}
		\item \textbf{XGBoost}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{Before Tuning}: High true positives but some false positives, particularly for Class 1 and Class 2.
        \item \textbf{After Tuning}: Slight changes, indicating that the model was already performing well.
    \end{itemize}
\end{itemize}

Next we will look at the ROC-AUC and Precision-Recall curves.

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_LR.png}
    \caption{Precision-Recall and ROC-AUC curves for Logistic Regression}
    \label{ra_pr_LR}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_KNN.png}
    \caption{Precision-Recall and ROC-AUC curves for K-Nearest Neighbors (KNN)}
    \label{ra_pr_KNN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_RF.png}
    \caption{Precision-Recall and ROC-AUC curves for Random Forest}
    \label{ra_pr_RF}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_DT.png}
    \caption{Precision-Recall and ROC-AUC curves for Decision Tree}
    \label{ra_pr_DT}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_SVM.png}
    \caption{Precision-Recall and ROC-AUC curves for Support Vector Machine (SVM)}
    \label{ra_pr_SVM}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=14em]{ra_pr_XG.png}
    \caption{Precision-Recall and ROC-AUC curves for XGBoost}
    \label{ra_pr_XG}
\end{figure}

By analsing the above curves we infer the following:
\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.5em}
		\item \textbf{Logistic Regression}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{ROC-AUC}: The curves are tightly grouped, indicating good separation between the classes. The AUC values are relatively high for most classes, showing reliable performance in distinguishing between them.
					\item \textbf{Precision-Recall}: The curve is above 0.5, which indicates that the model is effective, particularly for Class 0 (with the highest AUC).
			\end{itemize}
		\item \textbf{KNN}:
		\vspace{-1.25em}	
		\begin{itemize}
					\setlength\itemsep{-1.5em}
					\item \textbf{ROC-AUC}: The curves for some classes are lower compared to others, indicating less effective separation for certain classes. AUC values for some classes are below 0.9.
					\item \textbf{Precision-Recall}: There is a noticeable drop for some classes, indicating that KNN struggles with precision at lower recall levels.
				\end{itemize}
		\item \textbf{Random Forest}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{ROC-AUC}: Shows strong performance with AUC close to 1.0 for most classes, suggesting that the model performs very well in distinguishing between the classes.
        \item \textbf{Precision-Recall}: High curves for most classes, indicating strong precision, especially for Class 0.
			\end{itemize}
		\item \textbf{Decision Tree}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{ROC-AUC}: Shows some weaknesses with lower AUC values, particularly for Class 2 and Class 3, indicating less reliable separation.
        \item \textbf{Precision-Recall}: The curves drop significantly for some classes, suggesting that the Decision Tree may not be as reliable in predicting positive cases for certain classes.
			\end{itemize}
		\item \textbf{SVM}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{ROC-AUC}: High AUC values for most classes, suggesting strong class separation. The performance is consistent across classes.
        \item \textbf{Precision-Recall}: Shows good performance for most classes, indicating a high level of precision across various thresholds.
			\end{itemize}
		\item \textbf{XGBoost}:
		\vspace{-1.25em}	
		\begin{itemize}
				\setlength\itemsep{-1.5em}
        \item \textbf{ROC-AUC}: Excellent performance with AUC close to 1.0 for most classes, suggesting that XGBoost is particularly strong at distinguishing between classes.
        \item \textbf{Precision-Recall}: Similar to Random Forest, XGBoost shows high precision-recall curves, indicating reliable prediction, particularly for Class 0 and Class 1.
    \end{itemize}
\end{itemize}

\section{Evaluation Metrics}
The initial model training resulted good accuracy during training but some inconsistencies in test performance, meaning there was overfitting.
\begin{table}[H]
    \centering
		\begin{tabular}{p{2.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1cm}p{1cm}}
    \toprule
    \textbf{Model} & \textbf{Accuracy Train} & \textbf{Accuracy Test} & \textbf{Precision Train} & \textbf{Precision Test} & \textbf{Recall Train} & \textbf{Recall Test} & \textbf{F1-Score Train} & \textbf{F1-Score Test}\\
    \toprule
    Logistic Regression & $0.9463$ & $0.9444$ & $0.9465$ & $0.9445$ & $0.9463$ & $0.9444$ & $0.9464$ & $0.9443$ \\
    \midrule
    KNN & $0.8580$ & $0.7222$ & $0.8608$ & $0.7401$ & $0.8580$ & $0.7222$ & $0.8588$ & $0.7279$ \\
    \midrule
    Random Forest & $1.0000$ & $0.8889$ & $1.0000$ & $0.8886$ & $1.0000$ & $0.8889$ & $1.0000$ & $0.8885$ \\
    \midrule
    Decision Tree & $1.0000$ & $0.8712$ & $1.0000$ & $0.8722$ & $1.0000$ & $0.8712$ & $1.0000$ & $0.8715$ \\
    \midrule
    SVM & $0.9545$ & $0.9066$ & $0.9549$ & $0.9084$ & $0.9545$ & $0.9066$ & $0.9546$ & $0.9070$ \\
    \midrule
    XGBoost & $1.0000$ & $0.9167$ & $1.0000$ & $0.9168$ & $1.0000$ & $0.9167$ & $1.0000$ & $0.9164$ \\
    \toprule
    \end{tabular}
    \caption{Model Performance Metrics}
		\label{tab:model_perf_mat}
\end{table}
\vspace{-1.5em}
\begin{enumerate}
    \setlength\itemsep{-1.5em}
    \item{\textbf{Logistic Regression:}} High test accuracy ($\sim 94.44\%$), indicating good generalization.
    \item{\textbf{KNN:}} Moderate performance on the test set ($\sim 72.22\%$), likely due to its sensitivity to the choice of n\_neighbors.
    \item{\textbf{Random Forest:}} Overfitting on the training set ($100\%$ accuracy) but reduced generalization ($\sim 87.88\%$ test accuracy).
    \item{\textbf{Decision Tree:}} Overfitting on the training set ($100\%$ accuracy) but reduced generalization ($\sim 86.11\%$ test accuracy).
    \item{\textbf{SVM:}} Consistently high performance with test accuracy ($\sim 90.66\%$).
    \item{\textbf{XGBoost:}} Consistently high performance with test accuracy ($\sim 91.67\%$).
\end{enumerate}
As we can see there are overfitting issues with some models. We used \pythoninline{GridSearchCV} to tune the hyperparameters to get better performance and got the following results:

\begin{table}[H]
	\centering
	\begin{tabular}{p{2.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1cm}p{1cm}}
	\toprule
	\textbf{Model} & \textbf{Accuracy Train} & \textbf{Precision Train} & \textbf{Recall Train} & \textbf{F1-Score Train} & \textbf{Accuracy Test} & \textbf{Precision Test} & \textbf{Recall Test} & \textbf{F1-Score Test} \\
	\toprule
	Logistic Regression & $0.9457$ & $0.9458$ & $0.9457$ & $0.9457$ & $0.9495$ & $0.9492$ & $0.9495$ & $0.9493$ \\
	\midrule
	KNN & $0.8592$ & $0.8615$ & $0.8592$ & $0.8596$ & $0.7626$ & $0.7790$ & $0.7626$ & $0.7670$ \\
	\midrule
	Random Forest & $1.0000$ & $1.0000$ & $1.0000$ & $1.0000$ & $0.8889$ & $0.8906$ & $0.8889$ & $0.8889$ \\
	\midrule
	Decision Tree & $0.9968$ & $0.9968$ & $0.9968$ & $0.9968$ & $0.8712$ & $0.8718$ & $0.8712$ & $0.8710$ \\
	\midrule
	SVM & $0.9476$ & $0.9477$ & $0.9476$ & $0.9476$ & $0.9520$ & $0.9518$ & $0.9520$ & $0.9518$ \\
	\midrule
	XGBoost & $0.9962$ & $0.9962$ & $0.9962$ & $0.9962$ & $0.9192$ & $0.9196$ & $0.9192$ & $0.9190$ \\
	\toprule
	\end{tabular}
	\caption{Model Performance Metrics after Hyperparameter Tuning}
	\label{tab:tuned_per_mat}
\end{table}

Time time taken for training the models was as follows:
\begin{table}[H]
	\centering
	\begin{tabular}{lcc}
	\toprule
	\textbf{Model} & \textbf{Time Taken (seconds)} \\
	\toprule
	Logistic Regression & $0.008992$ \\
	\midrule
	KNN & $0.001998$ \\
	\midrule
	Random Forest & $0.262442$ \\
	\midrule
	Decision Tree & $0.007005$ \\
	\midrule
	SVM & $0.237206$ \\
	\midrule
	XGBoost & $0.211886$ \\
	\toprule
	\end{tabular}
	\caption{Time Taken for Model Training}
	\label{tab:time_taken}
\end{table}
As we can see Logistic Regression, KNN and Decision Tree are the fastest to train while Random Forest, SVM and XGBoost take the most time. This is expected as Random Forest and SVM are more complex models and require more time to train.


\section{Comparison between different Models}
Comparing the matrices form Table \ref{tab:model_perf_mat} amd Table \ref{tab:tuned_per_mat} we can infer:
\vspace{-1.5em}
\begin{itemize}
	\setlength\itemsep{-1.5em}
	\item \textbf{Logistic Regression:} The performance metrics remained consistent after hyperparameter tuning, indicating that the model was already well-optimized.
	\item \textbf{KNN:} There was a significant improvement in test accuracy from $0.7222$ to $0.7790$, as well as improvements in precision, recall, and F1-score, suggesting that hyperparameter tuning effectively addressed underfitting.
	\item \textbf{Random Forest:} The test accuracy slightly decreased from $0.8889$ to $0.8906$, but the model maintained high performance, indicating that hyperparameter tuning helped in balancing the training and test metrics.
	\item \textbf{Decision Tree:} The performance metrics remained relatively stable, with a slight improvement in test precision and recall, indicating that hyperparameter tuning helped in reducing overfitting.
	\item \textbf{SVM:} The test accuracy improved from $0.9066$ to $0.9518$, along with improvements in precision, recall, and F1-score, suggesting that hyperparameter tuning significantly enhanced the model's performance.
	\item \textbf{XGBoost:} The test accuracy improved from $0.9167$ to $0.9196$, with consistent improvements in other metrics, indicating that hyperparameter tuning helped in maintaining high performance and efficient learning.
\end{itemize}

\chapter{Discussion}
\section{Analysis of Results}
\textbf{Logistic Regression} performed reliably, especially after minor improvements post-tuning. \textbf{KNN} and \textbf{Decision Tree} struggled with certain classes, as evident from their lower Precision-Recall curves. \textbf{XGBoost} and \textbf{SVM} consistently achieved high F1-scores, indicating a strong balance between precision and recall. 

\textbf{Random Forest}, \textbf{Decision Tree}, and \textbf{XGBoost} are inherently complex and hence prone to overfitting. \textbf{GridSearchCV} significantly improved the balance between training and test metrics for most models, particularly \textbf{KNN} and \textbf{SVM}.

\section{Anomalies and Unexpected Findings}
The dataset we chose did not have any class imbalances but there may have been challenges related to it, which can affect model predictions, particularly in underrepresented categories. \\
Also we found that among the 19 features, 13 were not much important such as 3G, 4G, front and primary camera quality, and screen resolution components. Removing redundant features improved model performance and reduced complexity. \\	
\section{Limitations}
\vspace{-0.5em}
\begin{itemize}
	\setlength\itemsep{-1.25em}
	\item  If certain price categories were underrepresented, the model might not perform equally well across all classes, potentially favoring more common categories.
	\item The impact of feature selection and engineering steps could vary, and further refinement might be needed to capture more complex relationships in the data.
	\item The results might not generalize well to unseen data if there were biases in the dataset or if the training set wasn't diverse enough.
\end{itemize}

\chapter{Conclusion}
\section{Key Takeaways}
The key takeaways from this project are:
\vspace{-1.25em}
\begin{itemize}
	\setlength\itemsep{-1.05em}
	\item The project successfully developed a classification model capable of categorizing smartphones into distinct price segments (Low, Medium, High, Very High) using a variety of machine learning algorithms.
	\item A thorough feature analysis revealed the most influential factors for predicting smartphone prices, such as RAM, battery power, and camera quality.
	\item The systematic feature selection process, including normalization and encoding, highlighted critical features that contributed significantly to model accuracy.
	\item Effective feature engineering, including normalization of screen and pixel areas, ensured a refined and optimized dataset for training.
	\item Various machine learning models were implemented, tuned, and evaluated. Metrics such as accuracy, precision, and recall were used to compare the models.
	\item The project provided clear insights into which algorithms were more effective for this classification task. For example, ensemble methods like the Voting Classifier generally outperformed individual classifiers.
	\item The utilization of metrics like confusion matrices, ROC curves, and AUC scores provided a comprehensive understanding of the model’s accuracy and reliability.
	\item The project made methodological contributions by developing a replicable framework for smartphone price categorization, emphasizing feature analysis and multi-algorithm comparison.
	\item The workflow established in this project serves as a template for future predictive modeling challenges, particularly in consumer market analysis using machine learning techniques.
\end{itemize}
\section{Reflection on Objectives}
\vspace{-0.5em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{Robust Classification Model :}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item The project met its objective of building an accurate classification model, using multiple algorithms and optimizing them for high predictive accuracy.
		      \item The most significant features influencing smartphone pricing were identified, meeting the goal of leveraging key variables for prediction.
	      \end{itemize}
	\item{\textbf{Feature Analysis and Selection :}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item A comprehensive feature analysis was performed, and a systematic approach to feature selection was developed, successfully identifying the most influential smartphone attributes.
		      \item The selection of relevant features directly impacted the model’s performance, aligning with the project’s goals.
	      \end{itemize}
	\item{\textbf{Comparative Algorithm Performance :}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Multiple machine learning algorithms were tested, and their performances were compared, allowing for the selection of the most effective model for smartphone categorization.
		      \item The comparison revealed insights into which algorithms handled the dataset’s characteristics best, achieving the project’s objective of comparative analysis.
	      \end{itemize}
	\item{\textbf{Practical Applicability :}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item The practical utility of the model was demonstrated, showcasing how it could assist both manufacturers and consumers in making data-driven decisions, achieving this core objective.
	      \end{itemize}
	\item{\textbf{Methodological Contribution :}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item A replicable methodology was established, contributing to market analysis using machine learning. The project’s systematic approach, from feature analysis to model comparison, stands as a reference for similar challenges.
	      \end{itemize}
\end{enumerate}
\section{Future Work}
\vspace{-0.5em}
\begin{enumerate}
	\setlength\itemsep{-1.05em}
	\item{\textbf{Addressing Class Imbalances: }}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Future research could focus on balancing the dataset more effectively, employing techniques like SMOTE (Synthetic Minority Over-sampling Technique) to handle underrepresented categories.
		      \item Experimenting with different sampling strategies or cost-sensitive algorithms might yield better performance across all price segments.
	      \end{itemize}
	\item{\textbf{Advanced Feature Engineering: }}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Introducing more sophisticated feature engineering methods, such as polynomial features or interaction terms, might capture deeper relationships between variables.
		      \item Incorporating external data sources like user reviews or brand reputation could provide additional features for enhanced model predictions.
	      \end{itemize}
	\item{\textbf{Deep Learning Approaches:	}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Exploring deep learning models, such as neural networks, could help capture complex patterns in the data that traditional machine learning models might miss.
		      \item Fine-tuning deep learning algorithms might lead to better generalization and improved accuracy for smartphone price prediction.
	      \end{itemize}
	\item{\textbf{Model Interpretability:	:}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Enhancing model interpretability by using techniques like SHAP (Shapley Additive Explanations) or LIME (Local Interpretable Model-agnostic Explanations) can provide more insights into how predictions are made, especially for complex models.
	      \end{itemize}
	\item{\textbf{Expanding Practical Use Cases:	}}
	      \vspace{-1.65em}
	      \begin{itemize}
		      \setlength\itemsep{-1.5em}
		      \item Future work could extend beyond categorization, such as predicting the price value itself or identifying which features drive consumer satisfaction.
		      \item The developed model can be adapted to other product categories, providing a broader application in market analysis and consumer research.
	      \end{itemize}
\end{enumerate}

\renewcommand{\bibname}{References}
\begin{thebibliography}{9}
	\bibitem{hadsonml}
	Géron, Aurélien. \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}. 3rd ed., O'Reilly Media, 2023.

	\bibitem{asim2018}
	Asim, Muhammad, and Zafar Khan. "Mobile Price Class Prediction Using Machine Learning Techniques." \textit{International Journal of Computer Applications}, March 2018. DOI: \texttt{10.5120/ijca2018916555}.

	\bibitem{chandrashekhara2019}
	Chandrashekhara, K. T., and M. Thungamani. "Smartphone Price Prediction in Retail Industry Using Machine Learning Techniques." In \textit{Emerging Research in Electronics, Computer Science and Technology, Lecture Notes in Electrical Engineering 545}, edited by V. Sridhar et al., Springer Nature Singapore Pte Ltd., 2019. DOI: \texttt{10.1007/978-981-13-5802-9\_34}.

	\bibitem{ercan2023}
	Ercan \&  Şimşek, "Mobile Phone Price Classification Using Machine Learning." \textit{International Journal of Advanced Natural Sciences and Engineering Researches}, vol. 7, no. 4, 2023, pp. 458-462. DOI: \texttt{10.59287/ijanser.791}.

	\bibitem{abbasi2024}
	Abbasi, Muhammad Hasnain, Abdul Sajid, Muhammad Arshad Awan, and Ayeb Amani. "Predicting The Price Of Used Electronic Devices Using Machine Learning Techniques." \textit{International Journal of Computing and Related Technologies}, vol. 4, no. 1, 2024. Available at: \texttt{https://www.researchgate.net/publication/377526585}.

	\bibitem{gupta2020}
	Gupta, Akash, and Suhasini Kottur. "Mobile Price Prediction by Its Features Using Predictive Model of Machine Learning." \textit{UGC Care Journal}, August 2020. DOI: \texttt{10.13140/RG.2.2.20054.52800}.

	\bibitem{kumuda2021}
	Kumuda S, Vishal Karur, and Karthick Balaje S. E. "Prediction of Mobile Model Price Using Machine Learning Techniques." \textit{International Journal of Engineering and Advanced Technology (IJEAT)}, vol. 11, no. 1, October 2021. DOI: \texttt{10.35940/ijeat.A3219.1011121}.

\end{thebibliography}
\addcontentsline{toc}{chapter}{References}

\end{document}