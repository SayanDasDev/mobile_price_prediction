\documentclass[14pt, aspectratio=169]{beamer}
\usetheme{copenhagen}
\usecolortheme{seahorse}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{headline}{} 
\setbeamertemplate{footline}{} 
\setbeamercovered{transparent} 

\usepackage{amsmath, amsfonts, amssymb, enumerate, tfrupee, xcolor, lipsum, minted, algorithm, algpseudocode, algorithmicx, float, hyperref, booktabs, graphicx, array, multicol, changepage}

\title{A Classification Based approach for predicting Smartphone Price Categories}
\author{Team: \textbf{Insight Engineers} \\  \textbf{Sayan Das} - B2430035 \and \textbf{Raihan Uddin} - B2430070 \\ Supervisor: \textbf{Br. Bhaswarachaitanya} (Tamal Maharaj)}
\date{\today}

\begin{document}
\maketitle

\begin{frame}{Outline}
  \tableofcontents
\end{frame}



\section{Introduction}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Introduction}
  \normalsize
\end{frame}
\begin{frame}{Background}
    \begin{itemize}
      \item The smartphone industry experiences continuous technological innovations, with manufacturers introducing advanced features.
      \item Multiple global players, such as Apple, Samsung, and Xiaomi, vie for market share, leading to frequent product launches and \textbf{pricing battles}.
      \item Consumers demand \textbf{value for money}, with preferences shifting toward devices offering high performance at competitive prices.
  \end{itemize}
\end{frame}
\begin{frame}{Motivation}
  We hope our model will help:
  \vspace{1.25em}
  \begin{itemize}
	\setlength\itemsep{1em}
    \item<1-> Simplify pricing strategies for \textbf{manufacturers}
    \item<2-> Increase pricing transparency for \textbf{consumers}
  \end{itemize}
\end{frame}

\section{Objectives and Scope}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Objectives and Scope}
  \normalsize
\end{frame}
\begin{frame}{Objectives}
  \begin{itemize}
	\setlength\itemsep{0.75em}
    \item<1-> Develop a Robust Classification Model to predict smartphone price categories
    \item<2-> Determine the most influential factors behind smartphone pricing
  \end{itemize}
\end{frame}

\begin{frame}{Scope}
  \begin{block}{What This Project Does Cover}
  \begin{itemize}
	\setlength\itemsep{0.75em}
    \item<1-> We will train multiple classification models to \textbf{predict the price range} of smartphones.
    \item<2-> We will analyze the \textbf{feature importance} to determine the most influential factors behind smartphone pricing. 
    \item<3-> We will \textbf{compare the performance} of various machine learning algorithms.
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Scope}
  \begin{block}{What This Project Does Not Cover}
    \begin{itemize}
      \item<1-> Does not predict \textbf{the exact price of smartphones}, only the price category.
      \item<2-> Does not include \textbf{real-time} data updates or predictions.
      \item<3-> Does not cover hardware or software \textbf{implementation details} of the smartphone features.
      \item<4-> Does not account for \textbf{market trends} or \textbf{external factors} influencing smartphone prices.
      \item<4-> Does not concern with \textbf{development} details.
    \end{itemize}
  \end{block}
\end{frame}


\section{Dataset Description}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Dataset Description}
  \normalsize
\end{frame}
\begin{frame}{Source}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{kaggle.png}
  \end{figure}
  Link - \footnotesize \href{https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification?select=train.csv}{https://www.kaggle.com/datasets/iabhishekofficial/smartphone-price-classification}\\
  \normalsize
  
  The dataset is publicly available and contains \textbf{2000 smartphone entries} with \textbf{19 feature variables} and \textbf{1 target variable} representing \texttt{price\_range}.
\end{frame}
\begin{frame}{Features}
  \vspace{-0.25em}
  \scriptsize
  \begin{table}[H]
    \begin{tabular}{lll}
      \toprule
      \textbf{Feature Name}   & \textbf{Description}  & \textbf{Type}  \\
      \toprule
      \texttt{battery\_power} & battery capacity in mAh   & Numerical  \\
      \texttt{clock\_speed}   & speed at which processor executes instructions & Numerical \\
      \texttt{fc}             & front Camera Megapixels & Numerical \\
      \texttt{pc}             & primary Camera Megapixels & Numerical \\
      \texttt{int\_memory}    & internal Memory capacity & Numerical \\
      \texttt{m\_dep}         & smartphone Depth in cm & Numerical \\
      \texttt{mobile\_wt}     & weight of the smartphone & Numerical \\
      \texttt{n\_cores}       & number of cores in processor & Numerical \\
      \texttt{px\_height}     & pixel Resolution Height & Numerical \\
      \texttt{px\_width}      & pixel Resolution Width & Numerical \\
      \texttt{ram}            & RAM in MB & Numerical \\
      \texttt{sc\_h}          & screen Height in cm & Numerical \\
      \texttt{sc\_w}          & screen Width in cm & Numerical \\
      \texttt{talk\_time}     & longest time that a single battery charge will last over a call & Numerical \\
      \texttt{blue}           & has bluetooth or not  & Categorical   \\
      \texttt{dual\_sim}      & has dual sim support or not & Categorical \\
      \texttt{four\_g}        & has 4G or not & Categorical \\
      \texttt{three\_g}       & has 3G or not & Categorical \\
      \texttt{wifi}           & has wifi or not & Categorical \\
      \texttt{touch\_screen}  & has touch screen or not & Categorical \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Target Variable}
  \begin{itemize}
  \setlength\itemsep{0.5em}
  \item The target variable \texttt{price\_range} is \textbf{categorical} with \textbf{4 classes}.
  \begin{itemize}
  \setlength\itemsep{0.25em} \small
      \item \texttt{0} - Low Cost - \textbf{Budget} Smartphones
      \item \texttt{1} - Medium Cost - \textbf{Mid-Range} Smartphones
      \item \texttt{2} - High Cost - \textbf{High-End} Smartphones
      \item \texttt{3} - Very High Cost - \textbf{Flagship} Smartphones 
    \end{itemize}
    \normalsize
  \end{itemize}
\end{frame}

\section{Data Preprocessing}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Data Preprocessing}
  \normalsize
\end{frame}

\begin{frame}[allowframebreaks]{Data Cleaning - Handling Missing Values}
  \begin{figure}[H]
    \centering
    \includegraphics[height=0.3\textwidth]{missing_val.png}
  \end{figure}
  \begin{minipage}{0.475\textwidth}
    \begin{block}{}
      \textbf{Significance:} \small
      Machine learning models often require complete data to function correctly. Missing values can lead to errors.
    \end{block}
  \end{minipage}
  \hfill
  \begin{minipage}{0.475\textwidth}
    \begin{block}{}
      \textbf{Observation:} \small
      There are no missing values in the dataset.
    \end{block}
  \end{minipage}
\end{frame}

\begin{frame}[containsverbatim]{Data Cleaning - Handling Duplicate Values}
  \begin{minted}{python}
    df.duplicated().sum()
  \end{minted}
  \vspace{1em}
  \begin{minted}{python}
    np.int64(0)
  \end{minted}
  $ $\\
  \begin{minipage}{0.8\textwidth}
    \begin{block}{}
      \textbf{Significance:} \small
      Duplicate entries can distort the true representation of the data, leading to \textbf{bias}.
    \end{block}
    \begin{block}{}
      \textbf{Observation:} \small
      There are no duplicate values in the dataset.
    \end{block}
  \end{minipage}
\end{frame}

\begin{frame}[allowframebreaks, containsverbatim]{Data Cleaning - Handling Invalid Values}
  \begin{minted}{python}
negative_counts = df.apply(lambda x: (x < 0).sum())
print(negative_counts)
  \end{minted}
  \begin{minipage}{0.2\textwidth}
    
  \tiny
  \vspace{0.3em}
  \begin{minted}{python}
  battery_power    0
  blue             0
  clock_speed      0
  dual_sim         0
  fc               0
  four_g           0
  int_memory       0
  m_dep            0
  mobile_wt        0
  n_cores          0
  pc               0
  px_height        0
  px_width         0
  ram              0
  sc_h             0
  sc_w             0
  talk_time        0
  three_g          0
  touch_screen     0
  wifi             0
  price_range      0
  dtype: int64

  \end{minted}
  \normalsize
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
  \begin{block}{}
    \textbf{Significance:} \small
    None of the features can have negative values.
  \end{block}
  \begin{block}{}
    \textbf{Observation:} \small
    There are no negative values in the dataset.
  \end{block}
  
\end{minipage}

  \begin{minted}{python}
zero_counts = df.apply(lambda x: (x == 0).sum())
print(zero_counts)
  \end{minted}
  \begin{minipage}{0.2\textwidth}
    
  \tiny
  \vspace{0.3em}
  \begin{minted}{python}
battery_power       0
blue             1010
clock_speed         0
dual_sim          981
fc                474
four_g            957
int_memory          0
m_dep               0
mobile_wt           0
n_cores             0
pc                101
px_height           2
px_width            0
ram                 0
sc_h                0
sc_w              180
talk_time           0
three_g           477
touch_screen      994
wifi              986
price_range       500
dtype: int64

  \end{minted}
  \normalsize
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
  \small
  \begin{block}{}
    \textbf{Significance:} \small
    Most of the numerical features can not be zero except \texttt{fc} and \texttt{pc}. These two being zero means the phone does not have a front or primary camera.
  \end{block}
  \begin{block}{}
    \textbf{Observation:} \small
    \texttt{px\_height} and \texttt{sc\_w} are have 2 and 180 zero values respectively.
  \end{block}
  \begin{block}{}
    \textbf{Action:} \small
    We replaced these zero values with the mean of the respective features.
  \end{block}
\end{minipage}
\small
\begin{minted}{python}
to_replace_with_mean = ['sc_w', 'px_height']

for feature in to_replace_with_mean:
  df[feature] = df[feature].replace(0, df[feature].mean())
\end{minted}
  \normalsize
\end{frame}

\begin{frame}[allowframebreaks]{Data Cleaning - Outlier Handling}
  \begin{minipage}{0.8\textwidth}
    \begin{block}{}
      \textbf{Significance:} \small
      \begin{itemize}
        \item Outliers can distort the true representation of data.
        \item Machine learning models can be sensitive to outliers.
      \end{itemize}
    \end{block}
  \end{minipage}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../report/box_plots_before.png}
  \end{figure}
\end{frame}

\begin{frame}{Data Cleaning - Outlier Handling (cont.)}
  \begin{minipage}{0.8\textwidth}
    \begin{block}{}
      \textbf{Observation:} \small
      The box plots revealed outliers in two features, \texttt{fc} and \texttt{px\_height}.
    \end{block}
  \end{minipage}
  \begin{minipage}{0.8\textwidth}
    \begin{block}{}
      \textbf{Action:} \small
      To identify these outliers, we will use the IQR (Interquartile Range) method and remove extreme values.
    \end{block}
  \end{minipage}
  \footnotesize
  \[
  \text{IQR} = Q3 - Q1
  \]
  Once the IQR is calculated, outliers are identified using the following bounds:
  \[
  \text{Lower Bound} = Q1 - 1.5 \times \text{IQR} \quad \quad \quad \text{Upper Bound} = Q3 + 1.5 \times \text{IQR}
  \]
  Any data point that falls below the lower bound or above the upper bound is considered an outlier.
  \normalsize
\end{frame}

\begin{frame}[containsverbatim]{Data Cleaning - Outlier Handling (cont.)}
\footnotesize
\begin{minted}{python}
def remove_outliers_iqr(data, column):
  Q1 = data[column].quantile(0.25)
  Q3 = data[column].quantile(0.75)
  IQR = Q3 - Q1

  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR

  filtered_data = data[(data[column] >= lower_bound) &
                       (data[column] <= upper_bound)]
  return filtered_data

df = remove_outliers_iqr(df, 'fc')
df = remove_outliers_iqr(df, 'px_height')
\end{minted}

\end{frame}

\begin{frame}[containsverbatim]{Data Cleaning - Outlier Handling (cont.)}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{../report/box_plots_after.png}
  \end{figure}
\end{frame}
\begin{frame}{Data Cleaning - Checking for Class Imbalance}
  \begin{figure}[H]
    \centering
    \includegraphics[height=0.3\textwidth]{../report/imbalance_check.png}
  \end{figure}
  \begin{minipage}{0.475\textwidth}
    \begin{block}{}
      \textbf{Significance:} \small
      Imbalanced dataset makes the model biased towards the majority class.
    \end{block}
  \end{minipage}
  \hfill
  \begin{minipage}{0.475\textwidth}
    \begin{block}{}
      \textbf{Observation:} \small
      There are no class imbalance in the dataset.
    \end{block}
  \end{minipage}
\end{frame}


\begin{frame}[allowframebreaks, containsverbatim]{Data Cleaning - Correlation Analysis}
  \begin{minipage}{0.3\textwidth}    
    \begin{block}{}
      \textbf{Significance:} \small
      Correlation analysis helps identify relationships between features. It can help in feature engineering.
    \end{block}
    \normalsize
  \end{minipage}
  \hfill
  \begin{minipage}{0.6\textwidth}
    \begin{figure}[H]
      \centering
      \includegraphics[width=0.98\textwidth]{../report/corr_viz.png}
    \end{figure}
  \end{minipage}

  \begin{block}{}
    \textbf{Observation:} \small
    \begin{enumerate}
      \setlength\itemsep{.05em}
      \item{\textbf{ram and price\_range :}} Higher RAM capacity leads to higher price range.
      \item{\textbf{three\_g and four\_g :}} A high correlation here suggests that devices with 4G almost always support 3G, making one of these features redundant.
      \item{\textbf{fc and pc :}} These features are correlated, as better primary cameras often accompany better front cameras.
      \item{\textbf{px\_height and px\_width :}} These are components of screen resolution and are naturally correlated.
      \item{\textbf{sc\_h and sc\_w :}} These are also naturally correlated.
    \end{enumerate}
  \end{block}
  
  \begin{block}{}
    \textbf{Action:} \small
    \begin{enumerate}
      \setlength\itemsep{.05em}
      \item{\textbf{three\_g and four\_g :}} \texttt{three\_g} was removed from the dataset.
      \item{\textbf{fc and pc :}} \texttt{fc} was removed from the dataset.
      \item{\textbf{px\_height and px\_width :}} They were combined to form a new feature \texttt{px\_area = px\_height * px\_width}.
      \item{\textbf{sc\_h and sc\_w :}} They were combined to form a new feature \texttt{screen\_area = sc\_h * sc\_w}.
    \end{enumerate}
  \end{block}

  \begin{minipage}{0.6\textwidth}
    \begin{figure}[H]
      \centering
      \includegraphics[width=0.98\textwidth]{../report/corr_viz_after_fe.png}
    \end{figure}
  \end{minipage}
  \hfill
  \begin{minipage}{0.3\textwidth} \small
    Correlation Matrix after feature engineering
    \normalsize
  \end{minipage}
\end{frame}

\begin{frame}[allowframebreaks, containsverbatim]{Feature Selection}
  \begin{block}{}
    \textbf{Significance:} \small
    By selecting the most relevant features, the model can focus on the \textbf{most important} information. Moreover, including irrelevant or redundant features can cause the model to \textbf{overfit} the training data. Also, fewer features mean \textbf{less data to process}.
  \end{block}
  \begin{block}{}
    \textbf{Action:} \small
    In our project, we used the ANOVA F-test (Analysis of Variance) method to evaluate each feature's relationship with the target variable, price\_range, and select features that are statistically significant. The threshold for selection is a p-value of less than $0.1$ i.e. a $90\%$ confidence level.
  \end{block}
\footnotesize
  \begin{minted}{python}
y = df.pop('price_range')
X = df
feature_selector = SelectKBest(f_classif, k='all')  
X_selected = feature_selector.fit_transform(X, y)
p_values = feature_selector.pvalues_
f_scores = feature_selector.scores_

selected_features = X.columns[p_values < 0.1]
  \end{minted}
  \normalsize
  \begin{block}{}
    \textbf{Outcome:} \small
    The selected features are: \\
    \texttt{'battery\_power', 'int\_memory', 'mobile\_wt', 'n\_cores', 'ram', 'px\_area', 'screen\_area'}
  \end{block}
\end{frame}

\begin{frame}{Train-Test Split}
  \begin{block}{}
    \textbf{Significance:} \small
    Splitting the dataset into training and testing sets allows us to evaluate model performance and ensure that the model generalizes well to unseen data.
  \end{block}
  \begin{block}{}
    \textbf{Action:} \small
    We used \texttt{train\_test\_split} from \texttt{sklearn.model\_selection} to split the dataset into 80\% training and 20\% testing sets.
  \end{block}
  \begin{block}{}
    \textbf{Outcome:} \small
    The train-test split resulted in 1584 samples with 7 features for training and 396 samples with 7 features for testing, with corresponding target arrays of 1584 and 396 elements, respectively.
  \end{block}
\end{frame}

\begin{frame}{Scaling}
  \begin{block}{}
    \textbf{Significance:} \small
    Scaling ensures that all feature values are normalized, which improves the performance and convergence of many machine learning algorithms.
  \end{block}
  \begin{block}{}
    \textbf{Action:} \small
    We used the \texttt{StandardScaler} from \texttt{sklearn.preprocessing} to scale the feature values.
  \end{block}
  \begin{block}{}
    \textbf{Outcome:} \small
    The dataset was successfully scaled, resulting in normalized feature values that contribute equally to the model training process.
  \end{block}
\end{frame}

\section{Methodology and Analysis}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Methodology and Analysis}
  \normalsize
\end{frame}
\begin{frame}{Tools and Libraries}
  \begin{multicols}{2}
  \begin{enumerate}
    \setlength\itemsep{.25em}
    \item Python
    \item Jupyter Notebook
    \scriptsize
      \item Data Handling Libraries:
          \begin{enumerate}
              \item Pandas
              \item Numpy
          \end{enumerate}
      \item{\textbf{Visualization Libraries: }}
          \begin{enumerate}
              \item Matplotlib
              \item Seaborn
          \end{enumerate}
      \item{\textbf{Machine Learning Libraries: }}
          \begin{enumerate}
              \item Scikit-learn
              \item XGBoost
          \end{enumerate}
      \item{\textbf{Other Libraries: }}
          \begin{enumerate}
              \item TQDM
              \item Time
          \end{enumerate}
  \end{enumerate}
\end{multicols}
\end{frame}
\begin{frame}{Models Development}
  We trained 6 models based on following algorithms:
  \begin{enumerate}
    \setlength\itemsep{.25em}
    \item Logistic Regression
    \item K-Nearest Neighbors
    \item Decision Tree
    \item Random Forest
    \item Support Vector Machine
    \item XGBoost
  \end{enumerate}
\end{frame}
\begin{frame}[allowframebreaks, containsverbatim]{Initial Training}
  \small
  \begin{minted}{python}
models = {
  'Logistic Regression': LogisticRegression(max_iter=1000),
  'KNN': KNeighborsClassifier(),
  'Random Forest': RandomForestClassifier(),
  'Decision Tree': DecisionTreeClassifier(),
  'SVM': SVC(probability=True),
  'XGBoost': XGBClassifier(eval_metric='mlogloss'),
}
  \end{minted}
  \scriptsize

\begin{table}[H]
  \centering
  \begin{tabular}{p{1.5cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{0.75cm}p{0.75cm}}
  \toprule
  \textbf{Model} & \textbf{Accuracy Train} & \textbf{Accuracy Test} & \textbf{Precision Train} & \textbf{Precision Test} & \textbf{Recall Train} & \textbf{Recall Test} & \textbf{F1-Score Train} & \textbf{F1-Score Test}\\
  \toprule
  Logistic Regression & $0.9463$ & $0.9444$ & $0.9465$ & $0.9445$ & $0.9463$ & $0.9444$ & $0.9464$ & $0.9443$ \\
  \midrule
  KNN & $0.8580$ & $0.7222$ & $0.8608$ & $0.7401$ & $0.8580$ & $0.7222$ & $0.8588$ & $0.7279$ \\
  \midrule
  Random Forest & $1.0000$ & $0.8889$ & $1.0000$ & $0.8886$ & $1.0000$ & $0.8889$ & $1.0000$ & $0.8885$ \\
  \midrule
  Decision Tree & $1.0000$ & $0.8712$ & $1.0000$ & $0.8722$ & $1.0000$ & $0.8712$ & $1.0000$ & $0.8715$ \\
  \midrule
  SVM & $0.9545$ & $0.9066$ & $0.9549$ & $0.9084$ & $0.9545$ & $0.9066$ & $0.9546$ & $0.9070$ \\
  \midrule
  XGBoost & $1.0000$ & $0.9167$ & $1.0000$ & $0.9168$ & $1.0000$ & $0.9167$ & $1.0000$ & $0.9164$ \\
  \toprule
  \end{tabular}
  \caption{Initial Model Performance Metrics}
  \label{tab:model_perf_mat}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[height=0.5\textwidth]{../report/taining_time.png}
\end{figure}

\end{frame}

\begin{frame}[allowframebreaks, containsverbatim]{Hyperparameter Tuning}
Hyperparameter Tuning is done to optimize the models for better generalization and prediction accuracy. We used \texttt{GridSearchCV} to get the best parameters for each model. 
\scriptsize
\begin{multicols}{2}

\begin{minted}{python}

param_grids = {
  'Logistic Regression': {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'saga'],
    'penalty': ['l1', 'l2'],
    'max_iter': [500, 1000]
  },
  'KNN': {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
  },
  'Random Forest': {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
  },
  'Decision Tree': {
    'max_depth': [None, 10, 20],
    'criterion': ['gini', 'entropy']
  },
  'SVM': {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale']
  },
  'XGBoost': {
    'learning_rate': [0.05, 0.1],
    'n_estimators': [100, 200],
    'max_depth': [3, 6]
  },
}
\end{minted}
\end{multicols}
\begin{minted}{python}
for model_name, model in models.items():
  pipeline = Pipeline([
      ('scaler', StandardScaler()), 
      ('classifier', model)         
  ])
  param_grid = {
      f'classifier__{key}': value for key, value in param_grids[model_name].items()
  }
  grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', 
                              verbose=1, error_score='raise')  
  grid_search.fit(X_train, y_train) 

  best_params = grid_search.best_params_
  best_score = grid_search.best_score_
  best_pipeline = grid_search.best_estimator_
  best_model = best_pipeline.named_steps['classifier']

  y_train_pred = best_pipeline.predict(X_train)  
  y_test_pred = best_pipeline.predict(X_test)

\end{minted}

\begin{table}[H]
  \centering
  \begin{tabular}{p{1.5cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{0.75cm}p{0.75cm}}
  \toprule
  \textbf{Model} & \textbf{Accuracy Train} & \textbf{Accuracy Test} & \textbf{Precision Train} & \textbf{Precision Test} & \textbf{Recall Train} & \textbf{Recall Test} & \textbf{F1-Score Train} & \textbf{F1-Score Test} \\
  \toprule
  Logistic Regression & $0.9457$ & $0.9495$ & $0.9458$ & $0.9492$ & $0.9457$ & $0.9495$ & $0.9457$ & $0.9493$ \\
  \midrule
  KNN & $0.8592$ & $0.7626$ & $0.8615$ & $0.7790$ & $0.8592$ & $0.7626$ & $0.8596$ & $0.7670$ \\
  \midrule
  Random Forest & $1.0000$ & $0.8889$ & $1.0000$ & $0.8906$ & $1.0000$ & $0.8889$ & $1.0000$ & $0.8889$ \\
  \midrule
  Decision Tree & $0.9968$ & $0.8712$ & $0.9968$ & $0.8718$ & $0.9968$ & $0.8712$ & $0.9968$ & $0.8710$ \\
  \midrule
  SVM & $0.9476$ & $0.9520$ & $0.9477$ & $0.9518$ & $0.9476$ & $0.9520$ & $0.9476$ & $0.9518$ \\
  \midrule
  XGBoost & $0.9962$ & $0.9192$ & $0.9962$ & $0.9196$ & $0.9962$ & $0.9192$ & $0.9962$ & $0.9190$ \\
  \toprule
  \end{tabular}
  \caption{Model Performance Metrics after Hyperparameter Tuning}
  \label{tab:tuned_per_mat}
\end{table}

\end{frame}

\begin{frame}[allowframebreaks]{Analysis}
\begin{adjustwidth}{-0.75cm}{1.25cm} % Adjust the left and right margins by -1cm

  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_LR.png}
        \label{lc_LR}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_LR.png}
        \label{lc_tuned_LR}
    \end{minipage}
  \end{figure}
  
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_KNN.png}
        \label{lc_KNN}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_KNN.png}
        \label{lc_tuned_KNN}
    \end{minipage}
  \end{figure}
  
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_RF.png}
        \label{lc_RF}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_RF.png}
        \label{lc_tuned_RF}
    \end{minipage}
  \end{figure}
  
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_DT.png}
        \label{lc_DT}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_DT.png}
        \label{lc_tuned_DT}
    \end{minipage}
  \end{figure}
  
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_SVM.png}
        \label{lc_SVM}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_SVM.png}
        \label{lc_tuned_SVM}
    \end{minipage}
  \end{figure}
  
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_XG.png}
        \label{lc_XG}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/lc_tuned_XG.png}
        \label{lc_tuned_XG}
    \end{minipage}
  \end{figure}

  \begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_LR.png}
        \label{cmat_LR}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_LR.png}
        \label{cmat_tuned_LR}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{../report/cmat_KNN.png}
			\label{cmat_KNN}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_KNN.png}
        \label{cmat_tuned_KNN}
    \end{minipage}
    
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{../report/cmat_RF.png}
			\label{cmat_RF}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_RF.png}
        \label{cmat_tuned_RF}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{../report/cmat_DT.png}
			\label{cmat_DT}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_DT.png}
        \label{cmat_tuned_DT}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{../report/cmat_SVM.png}
			\label{cmat_SVM}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_SVM.png}
        \label{cmat_tuned_SVM}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
		\begin{minipage}{0.45\textwidth}
			\centering
			\includegraphics[height=12em]{../report/cmat_XG.png}
			\label{cmat_XG}
		\end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[height=12em]{../report/cmat_tuned_XG.png}
        \label{cmat_tuned_XG}
    \end{minipage}
\end{figure}

\end{adjustwidth}
\begin{adjustwidth}{-0.3cm}{0cm}
\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_LR.png}
  \label{ra_pr_LR}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_KNN.png}
  \label{ra_pr_KNN}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_RF.png}
  \label{ra_pr_RF}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_DT.png}
  \label{ra_pr_DT}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_SVM.png}
  \label{ra_pr_SVM}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[height=12em]{../report/ra_pr_XG.png}
  \label{ra_pr_XG}
\end{figure}
\end{adjustwidth}

\end{frame}

\section{Challenges and Learnings}

\begin{frame}{}
  \Huge
  \centering
  \textbf{Challenges and Learnings}
  \normalsize
\end{frame}

\begin{frame}[allowframebreaks]{Challanges and Learnings}
  \begin{block}{Challenges}
  \begin{itemize}
    \setlength\itemsep{1em}
    \item \textbf{Feature Engineering:} Feature engineering was challenging as we had to combine features to create new ones. This required understanding of both the dataset and domain knowledge.
    \item \textbf{Hyperparameter Tuning:} Tuning hyperparameters for each model was time-consuming and required a lot of time as well as computational resources.
  \end{itemize}
  \end{block}
  \footnotesize
  \begin{block}{Learnings}
    \begin{itemize}
      \item \textbf{Data Preprocessing:} The importance of thorough data cleaning, normalization, and scaling to improve model performance.
      \item \textbf{Model Evaluation:} The value of using multiple evaluation metrics (accuracy, precision, recall, F1-score) to get a comprehensive understanding of model performance.
      \item \textbf{Feature Engineering:} How creating new features and selecting the most relevant ones can significantly impact model accuracy.
      \item \textbf{Hyperparameter Tuning:} The impact of hyperparameter tuning on model performance and the importance of using techniques like GridSearchCV.
      \item \textbf{Model Comparison:} The benefits of comparing multiple machine learning algorithms to identify the best-performing model for a given task.
    \end{itemize}
  \end{block}
\end{frame}

\section{Conclusion}
\begin{frame}{}
  \Huge
  \centering
  \textbf{Conclusion}
  \normalsize
\end{frame}
\begin{frame}{Best Model}
  \begin{itemize}
    \setlength\itemsep{1em}
    \item \textbf{Best Model:} Logistic Regression
    \item \textbf{Second Best:} SVM
  \end{itemize}
\end{frame}

\begin{frame}{Most Important Features}
  
  \begin{figure}[H]
    \centering
    \includegraphics[height=14em]{feature_importance.png}
    \label{most_imp_feature}
  \end{figure}
\end{frame}
  
\begin{frame}{Future Work}
  \begin{itemize}
    \setlength\itemsep{1em}
    \item \textbf{Recommendation System:} This model can be improved upon to develop a recommendation system that takes inputs about features like RAM, megapixels, screen size, etc., and provides a possible price (budget) and even suggests smartphone models based on real-time data.
    \item \textbf{Deployment:} Deploying the model as a web application or API to provide real-time predictions and insights for users and businesses.
  \end{itemize}
\end{frame}

\section{References}
\begin{frame}{}
  \Huge
  \centering
  \textbf{References}
  \normalsize
\end{frame}
\begin{frame}{}
  \footnotesize
  \begin{thebibliography}{9}
    \bibitem{hadsonml}
    Géron, Aurélien. \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}. 3rd ed., O'Reilly Media, 2023.
  
    \bibitem{asim2018}
    Asim, Muhammad, and Zafar Khan. "Mobile Price Class Prediction Using Machine Learning Techniques." \textit{International Journal of Computer Applications}, March 2018. DOI: \texttt{10.5120/ijca2018916555}.
  
  \end{thebibliography}
\end{frame}

% \section{Acknowledgement}
% \begin{frame}{}
%   \Huge
%   \centering
%   \textbf{Acknowledgement}
%   \normalsize
% \end{frame}
% \begin{frame}{Acknowledgement}
%   \footnotesize
%   We would like to express our sincere gratitude to our supervisor, \textbf{Br. Bhaswarachaitanya} (Tamal Maharaj), Assistant Professor in the Department of Computer Science at \textit{Ramakrishna Mission Vivekananda Educational and Research Institute (RKMVERI)}, for his invaluable guidance, encouragement, and support throughout the course of this project. His expertise in machine learning and his insightful suggestions were instrumental in shaping this project.  

%   We would also like to thank \textit{Ramakrishna Mission Vivekananda Educational and Research Institute} for providing the necessary resources and facilities that enabled the successful completion of this project.

%   Lastly, we extend our heartfelt thanks to our batch mates for their constant encouragement and understanding during the challenging phases of this project.
% \end{frame}

\begin{frame}{}
  \Huge
  \centering
  \textbf{Thank You!}
\end{frame}

\end{document}
